"""
M√≥dulo de evaluaci√≥n detallada para predicci√≥n de disrupciones.

Este m√≥dulo implementa m√©tricas espec√≠ficas para el dominio de fusi√≥n nuclear,
donde la detecci√≥n de disrupciones (True Positive Rate alto) es cr√≠tica
mientras se minimizan falsas alarmas (False Positive Rate bajo).

M√©tricas implementadas:
    - Confusion Matrix
    - TPR (True Positive Rate / Recall / Sensitivity)
    - FPR (False Positive Rate)
    - Precision, F1 Score
    - ROC Curve y AUC
    - Precision-Recall Curve
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    confusion_matrix, 
    classification_report,
    roc_curve, 
    auc,
    precision_recall_curve,
    average_precision_score
)
from typing import Dict, Tuple, Optional
from pathlib import Path


def get_predictions(
    model: nn.Module,
    data_loader: torch.utils.data.DataLoader,
    device: torch.device
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Obtiene predicciones del modelo para todo un DataLoader.
    
    Esta funci√≥n ejecuta el modelo en modo evaluaci√≥n sobre todos los
    batches del DataLoader y recolecta las etiquetas reales, las
    predicciones de clase, y las probabilidades (para curvas ROC).
    
    Args:
        model: Modelo PyTorch entrenado
        data_loader: DataLoader con datos a evaluar
        device: Dispositivo (cuda/cpu)
        
    Returns:
        Tuple con tres arrays numpy:
            - y_true: Etiquetas reales [n_samples]
            - y_pred: Predicciones de clase (0 o 1) [n_samples]
            - y_proba: Probabilidades de clase positiva [n_samples]
    """
    model.eval()
    model.to(device)
    
    all_labels = []
    all_preds = []
    all_probs = []
    
    with torch.no_grad():
        for batch_x, batch_y in data_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            # Forward pass
            outputs = model(batch_x)
            
            # Probabilidades con softmax
            probs = torch.softmax(outputs, dim=1)
            
            # Predicciones (clase con mayor probabilidad)
            _, preds = torch.max(outputs, dim=1)
            
            # Guardar resultados
            all_labels.extend(batch_y.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            # Probabilidad de clase positiva (disrupci√≥n = clase 1)
            all_probs.extend(probs[:, 1].cpu().numpy())
    
    return (
        np.array(all_labels),
        np.array(all_preds),
        np.array(all_probs)
    )


def calculate_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_proba: np.ndarray
) -> Dict[str, float]:
    """
    Calcula todas las m√©tricas de clasificaci√≥n relevantes.
    
    Para predicci√≥n de disrupciones, las m√©tricas m√°s importantes son:
    - TPR (Recall): Queremos detectar TODAS las disrupciones
    - FPR: Queremos minimizar falsas alarmas
    
    Args:
        y_true: Etiquetas reales
        y_pred: Predicciones de clase
        y_proba: Probabilidades de clase positiva
        
    Returns:
        Diccionario con todas las m√©tricas calculadas
    """
    # Matriz de confusi√≥n
    # [[TN, FP], [FN, TP]] cuando labels=[0,1]
    cm = confusion_matrix(y_true, y_pred)
    
    # Extraer valores de la matriz
    # Para clasificaci√≥n binaria con clases 0 (normal) y 1 (disruptivo)
    tn, fp, fn, tp = cm.ravel()
    
    # Calcular m√©tricas
    # TPR = TP / (TP + FN) - Proporci√≥n de disrupciones detectadas
    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    
    # FPR = FP / (FP + TN) - Proporci√≥n de falsas alarmas
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
    
    # TNR (Specificity) = TN / (TN + FP)
    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0
    
    # Precision = TP / (TP + FP)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    
    # F1 Score = 2 * (precision * recall) / (precision + recall)
    f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0.0
    
    # Accuracy = (TP + TN) / Total
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    
    # AUC-ROC
    fpr_curve, tpr_curve, _ = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr_curve, tpr_curve)
    
    # Average Precision (√°rea bajo curva PR)
    avg_precision = average_precision_score(y_true, y_proba)
    
    return {
        'accuracy': accuracy,
        'tpr': tpr,           # Recall / Sensitivity
        'fpr': fpr,
        'tnr': tnr,           # Specificity  
        'precision': precision,
        'f1': f1,
        'roc_auc': roc_auc,
        'avg_precision': avg_precision,
        'tp': int(tp),
        'tn': int(tn),
        'fp': int(fp),
        'fn': int(fn),
        'total_samples': len(y_true),
        'total_disruptive': int(tp + fn),
        'total_normal': int(tn + fp)
    }


def plot_confusion_matrix(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    save_path: Optional[str] = None,
    figsize: Tuple[int, int] = (8, 6)
) -> plt.Figure:
    """
    Genera visualizaci√≥n de matriz de confusi√≥n.
    
    La matriz muestra:
    - Cuadrante superior izquierdo: TN (Normal correctamente clasificado)
    - Cuadrante superior derecho: FP (Falsa alarma)
    - Cuadrante inferior izquierdo: FN (Disrupci√≥n no detectada - CR√çTICO)
    - Cuadrante inferior derecho: TP (Disrupci√≥n detectada correctamente)
    
    Args:
        y_true: Etiquetas reales
        y_pred: Predicciones
        save_path: Ruta para guardar la figura (opcional)
        figsize: Tama√±o de la figura
        
    Returns:
        Objeto Figure de matplotlib
    """
    cm = confusion_matrix(y_true, y_pred)
    
    fig, ax = plt.subplots(figsize=figsize)
    
    # Crear heatmap
    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
    ax.figure.colorbar(im, ax=ax)
    
    # Etiquetas
    classes = ['Normal', 'Disruptivo']
    ax.set(
        xticks=np.arange(cm.shape[1]),
        yticks=np.arange(cm.shape[0]),
        xticklabels=classes,
        yticklabels=classes,
        ylabel='Etiqueta Real',
        xlabel='Predicci√≥n del Modelo',
        title='Matriz de Confusi√≥n\nPredicci√≥n de Disrupciones en Tokamak'
    )
    
    # Rotar etiquetas del eje x
    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')
    
    # A√±adir texto en cada celda
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            # Determinar nombre de la m√©trica
            if i == 0 and j == 0:
                label = f'TN\n{cm[i, j]}'
            elif i == 0 and j == 1:
                label = f'FP\n{cm[i, j]}'
            elif i == 1 and j == 0:
                label = f'FN\n{cm[i, j]}'
            else:
                label = f'TP\n{cm[i, j]}'
                
            ax.text(j, i, label,
                   ha='center', va='center', fontsize=14,
                   color='white' if cm[i, j] > thresh else 'black')
    
    fig.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Matriz de confusi√≥n guardada en: {save_path}")
    
    return fig


def plot_roc_curve(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    save_path: Optional[str] = None,
    figsize: Tuple[int, int] = (8, 6)
) -> plt.Figure:
    """
    Genera curva ROC (Receiver Operating Characteristic).
    
    La curva ROC muestra el trade-off entre TPR y FPR a diferentes
    umbrales de clasificaci√≥n. El √°rea bajo la curva (AUC) indica
    la capacidad discriminativa del modelo:
    - AUC = 1.0: Clasificador perfecto
    - AUC = 0.5: Clasificador aleatorio
    
    Args:
        y_true: Etiquetas reales
        y_proba: Probabilidades de clase positiva
        save_path: Ruta para guardar (opcional)
        figsize: Tama√±o de figura
        
    Returns:
        Objeto Figure de matplotlib
    """
    fpr, tpr, thresholds = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr, tpr)
    
    fig, ax = plt.subplots(figsize=figsize)
    
    # Curva ROC
    ax.plot(fpr, tpr, color='darkorange', lw=2,
            label=f'Curva ROC (AUC = {roc_auc:.3f})')
    
    # L√≠nea diagonal (clasificador aleatorio)
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
            label='Aleatorio (AUC = 0.5)')
    
    # Punto de operaci√≥n ideal (esquina superior izquierda)
    ax.scatter([0], [1], s=100, c='green', marker='*', zorder=5,
              label='Punto ideal (0, 1)')
    
    # Marcar targets del proyecto
    ax.axhline(y=0.90, color='red', linestyle=':', alpha=0.7,
               label='Target TPR > 90%')
    ax.axvline(x=0.10, color='red', linestyle=':', alpha=0.7,
               label='Target FPR < 10%')
    
    # Regi√≥n de operaci√≥n aceptable
    ax.fill_between([0, 0.10], [0.90, 0.90], [1, 1], alpha=0.1, color='green',
                    label='Zona objetivo')
    
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate (Tasa de Falsas Alarmas)')
    ax.set_ylabel('True Positive Rate (Tasa de Detecci√≥n)')
    ax.set_title('Curva ROC - Predicci√≥n de Disrupciones')
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)
    
    fig.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Curva ROC guardada en: {save_path}")
    
    return fig


def plot_precision_recall_curve(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    save_path: Optional[str] = None,
    figsize: Tuple[int, int] = (8, 6)
) -> plt.Figure:
    """
    Genera curva Precision-Recall.
    
    Esta curva es especialmente √∫til cuando las clases est√°n desbalanceadas.
    Muestra el trade-off entre precision y recall a diferentes umbrales.
    
    Args:
        y_true: Etiquetas reales
        y_proba: Probabilidades de clase positiva
        save_path: Ruta para guardar (opcional)
        figsize: Tama√±o de figura
        
    Returns:
        Objeto Figure de matplotlib
    """
    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
    avg_precision = average_precision_score(y_true, y_proba)
    
    fig, ax = plt.subplots(figsize=figsize)
    
    # Curva PR
    ax.plot(recall, precision, color='darkorange', lw=2,
            label=f'Curva PR (AP = {avg_precision:.3f})')
    
    # L√≠nea base (proporci√≥n de positivos)
    baseline = y_true.sum() / len(y_true)
    ax.axhline(y=baseline, color='navy', linestyle='--',
               label=f'Baseline ({baseline:.2f})')
    
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Recall (True Positive Rate)')
    ax.set_ylabel('Precision')
    ax.set_title('Curva Precision-Recall - Predicci√≥n de Disrupciones')
    ax.legend(loc='lower left')
    ax.grid(True, alpha=0.3)
    
    fig.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Curva PR guardada en: {save_path}")
    
    return fig


def print_evaluation_report(metrics: Dict[str, float]) -> None:
    """
    Imprime un reporte formateado de todas las m√©tricas.
    
    Incluye indicadores visuales de si cada m√©trica alcanza
    los targets del proyecto.
    
    Args:
        metrics: Diccionario de m√©tricas calculadas
    """
    print("\n" + "=" * 70)
    print("           REPORTE DE EVALUACI√ìN - PREDICCI√ìN DE DISRUPCIONES")
    print("=" * 70)
    
    # Resumen de datos
    print(f"\nüìä RESUMEN DEL DATASET")
    print(f"   Total de muestras:     {metrics['total_samples']}")
    print(f"   Disparos disruptivos:  {metrics['total_disruptive']}")
    print(f"   Disparos normales:     {metrics['total_normal']}")
    
    # Matriz de confusi√≥n resumida
    print(f"\nüìã MATRIZ DE CONFUSI√ìN")
    print(f"   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print(f"   ‚îÇ  TN: {metrics['tn']:4d}  ‚îÇ  FP: {metrics['fp']:4d}  ‚îÇ")
    print(f"   ‚îÇ  FN: {metrics['fn']:4d}  ‚îÇ  TP: {metrics['tp']:4d}  ‚îÇ")
    print(f"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
    
    # M√©tricas principales con targets
    print(f"\nüéØ M√âTRICAS PRINCIPALES")
    
    # TPR
    tpr_status = "‚úÖ" if metrics['tpr'] >= 0.90 else "‚ö†Ô∏è"
    print(f"   {tpr_status} TPR (Recall):     {metrics['tpr']:.4f} ({metrics['tpr']*100:.1f}%)  [Target: >90%]")
    
    # FPR
    fpr_status = "‚úÖ" if metrics['fpr'] <= 0.10 else "‚ö†Ô∏è"
    print(f"   {fpr_status} FPR:              {metrics['fpr']:.4f} ({metrics['fpr']*100:.1f}%)  [Target: <10%]")
    
    # Otras m√©tricas
    print(f"\nüìà M√âTRICAS ADICIONALES")
    print(f"   Accuracy:         {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.1f}%)")
    print(f"   Precision:        {metrics['precision']:.4f} ({metrics['precision']*100:.1f}%)")
    print(f"   F1 Score:         {metrics['f1']:.4f}")
    print(f"   Specificity:      {metrics['tnr']:.4f} ({metrics['tnr']*100:.1f}%)")
    print(f"   ROC-AUC:          {metrics['roc_auc']:.4f}")
    print(f"   Avg Precision:    {metrics['avg_precision']:.4f}")
    
    # Interpretaci√≥n
    print(f"\nüí° INTERPRETACI√ìN")
    if metrics['tpr'] >= 0.90 and metrics['fpr'] <= 0.10:
        print("   ‚úÖ El modelo cumple con los targets del proyecto.")
        print("   ‚úÖ Listo para siguiente fase (implementaci√≥n FNO).")
    elif metrics['tpr'] >= 0.90:
        print("   ‚úÖ Detecci√≥n de disrupciones excelente.")
        print("   ‚ö†Ô∏è Tasa de falsas alarmas por encima del target.")
    elif metrics['fpr'] <= 0.10:
        print("   ‚ö†Ô∏è Algunas disrupciones no detectadas (revisar FN).")
        print("   ‚úÖ Pocas falsas alarmas.")
    else:
        print("   ‚ö†Ô∏è Modelo necesita mejoras en ambas m√©tricas.")
    
    print("\n" + "=" * 70)


def evaluate_model(
    model: nn.Module,
    data_loader: torch.utils.data.DataLoader,
    device: torch.device,
    save_dir: Optional[str] = None,
    plot: bool = True
) -> Dict[str, float]:
    """
    Funci√≥n principal de evaluaci√≥n completa.
    
    Ejecuta el pipeline completo de evaluaci√≥n:
    1. Obtiene predicciones del modelo
    2. Calcula todas las m√©tricas
    3. Genera visualizaciones (opcional)
    4. Imprime reporte formateado
    
    Args:
        model: Modelo PyTorch entrenado
        data_loader: DataLoader con datos de evaluaci√≥n
        device: Dispositivo (cuda/cpu)
        save_dir: Directorio para guardar plots (opcional)
        plot: Si generar visualizaciones
        
    Returns:
        Diccionario con todas las m√©tricas
        
    Example:
        >>> model = BaselineCNN(in_channels=5, num_classes=2)
        >>> model.load_state_dict(torch.load('results/best_model.pt'))
        >>> metrics = evaluate_model(model, val_loader, device, 'results/')
    """
    print("\nüîÑ Ejecutando evaluaci√≥n del modelo...")
    
    # Obtener predicciones
    y_true, y_pred, y_proba = get_predictions(model, data_loader, device)
    
    # Calcular m√©tricas
    metrics = calculate_metrics(y_true, y_pred, y_proba)
    
    # Crear directorio si no existe
    if save_dir:
        Path(save_dir).mkdir(parents=True, exist_ok=True)
    
    # Generar visualizaciones
    if plot:
        # Matriz de confusi√≥n
        cm_path = f"{save_dir}/confusion_matrix.png" if save_dir else None
        plot_confusion_matrix(y_true, y_pred, cm_path)
        
        # Curva ROC
        roc_path = f"{save_dir}/roc_curve.png" if save_dir else None
        plot_roc_curve(y_true, y_proba, roc_path)
        
        # Curva PR
        pr_path = f"{save_dir}/precision_recall_curve.png" if save_dir else None
        plot_precision_recall_curve(y_true, y_proba, pr_path)
    
    # Imprimir reporte
    print_evaluation_report(metrics)
    
    return metrics


# =============================================================================
# Test del m√≥dulo
# =============================================================================
if __name__ == "__main__":
    # Test con datos sint√©ticos
    print("Test del m√≥dulo de evaluaci√≥n")
    print("-" * 40)
    
    # Simular predicciones
    np.random.seed(42)
    n_samples = 200
    
    # Crear datos de prueba
    y_true = np.array([0] * 100 + [1] * 100)  # 100 normal, 100 disruptivo
    
    # Simular un modelo bueno (90% correcto)
    y_pred = y_true.copy()
    # Introducir algunos errores
    errors = np.random.choice(n_samples, size=20, replace=False)
    y_pred[errors] = 1 - y_pred[errors]
    
    # Simular probabilidades
    y_proba = np.where(y_true == 1, 
                       np.random.uniform(0.6, 1.0, n_samples),
                       np.random.uniform(0.0, 0.4, n_samples))
    
    # Calcular m√©tricas
    metrics = calculate_metrics(y_true, y_pred, y_proba)
    
    # Imprimir reporte
    print_evaluation_report(metrics)
    
    print("\n‚úÖ M√≥dulo de evaluaci√≥n funciona correctamente")